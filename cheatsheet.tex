% !TeX program = xelatex
\documentclass[UTF8,a4paper]{article}
\usepackage{ctex}
\usepackage{cheatsheet}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{ulem}
\usepackage{bm}
\usepackage{centernot}
\usepackage{mathrsfs}


\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=0pt, leftmargin=*}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=0pt, leftmargin=*}
\setitemize[2]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=0pt, leftmargin=*}
\setdescription{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=0pt, leftmargin=*}

\begin{document}
\begin{multicols}{3}

\begin{cheatsheetblock}{Multiplication Laws}
    matrix multiplication,
    \begin{itemize}
        \item $(\bm{A} \bm{B}) \bm{C} = \bm{A} (\bm{B} \bm{C})$ \hfill Associativity (结合律)
        \item $(\bm{A} + \bm{B}) \bm{C} = \bm{AC} + \bm{BC}$ \hfill Distributivity (分配律)
        \item $\bm{IA}=\bm{AI}=\bm{A}$ \hfill Multiplication with the identity matrix
        \item $\bm{AB} \neq \bm{AB}$ \hfill \sout{Commutativity}
    \end{itemize}
    scalar multiplication,
    \begin{itemize}
        \item $\lambda \psi \bm{C} = \lambda (\psi \bm{C})$ \hfill Associativity (结合律)
        \item $\lambda (\bm{BC}) = (\lambda \bm{B}) \bm{C} = \bm{B} (\lambda \bm{C}) = (\bm{BC})\lambda$ \hfill Associativity (结合律)
        \item $(\lambda \bm{C})^{\top} = \lambda \bm{C}^{\top}$
        \item $(\lambda + \psi) \bm{C} = \lambda\bm{C}+ \psi \bm{C}$  \hfill Distributivity (分配律)
        \item $\lambda (\bm{B} + \bm{C}) = \lambda\bm{B} +\lambda \bm{C}$  \hfill Distributivity (分配律)
    \end{itemize}
    symmetry: given $\bm{A}, \bm{B}$ is symmetrical, $\bm{A} + \bm{B}$ is symmetrical. $\bm{AB}$ could be NOT. \\
    $\bm{AB} = 0  \centernot\implies \bm{A} = 0 \text{ or } \bm{B} = 0$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Inverse and Transpose}
    \begin{itemize}
        \item $\bm{A}\bm{A}^{-1} = \bm{I} = \bm{A}^{-1}\bm{A}$
        \item $(\bm{AB})^{-1} = \bm{B}^{-1} \bm{A}^{-1}$
        \item $(\bm{A}^{\top})^\top = \bm{A}$
        \item $(\bm{A} + \bm{B})^\top = \bm{A}^{\top} + \bm{B}^{\top}$
        \item $(\bm{AB})^{\top} = \bm{B}^{\top} \bm{A} ^{\top}$
        \item $(\bm{A} + \bm{B})^{-1} = \bm{A}^{-1} + \bm{B}^{-1} - \bm{A}^{-1} (\bm{A}^{-1} + \bm{B}^{-1})^{-1} \bm{A}^{-1}$
    \end{itemize}
\end{cheatsheetblock}

\begin{cheatsheetblock}{Matrix Problems}
    \begin{itemize}
        \item \textbf{Solve Equations}: rref; condition of no solution; choose free variables, get the result.
        \item \textbf{Find Inv}: $ [\bm{A} | \bm{I}_{n}] \rightsquigarrow [\bm{I}_{n} | \bm{A}^{-1}]$
    \end{itemize}
\end{cheatsheetblock}

\begin{cheatsheetblock}{Group}
    \begin{itemize}
        \item Closure \hfill$\forall x,y\in\mathcal{G}:x\otimes y\in\mathcal{G}$
        \item Associativity \hfill $\forall x,y,z\in\mathcal{G}:(x\otimes y)\otimes z=x\otimes(y\otimes z)$
        \item Neutral element \hfill $\exists e\in\mathcal{G}\forall x\in\mathcal{G}:x\otimes e=x\mathrm{~and~}e\otimes x=x$
        \item Inverse element, $x^{-1}$ \hfill $\forall x\in\mathcal{G}\exists y\in\mathcal{G}:x\otimes y=e\mathrm{~and~}y\otimes x=e$
    \end{itemize}
    Abelian group: group with commutativity \hfill $x\otimes y=y\otimes x$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Vector Space}
    $V=(\mathcal{V},+,\cdot)$ \hfill inner: $+:\mathcal{V}\times\mathcal{V}\to\mathcal{V}$ \hfill outer: $\cdot:\mathbb{R}\times\mathcal{V}\to\mathcal{V}$
    \begin{itemize}
        \item $(\mathcal{V},+)$ is an Abelian group
        \item distributivity: $\lambda\cdot(\boldsymbol{x}+\boldsymbol{y})=\lambda\cdot\boldsymbol{x}+\lambda\cdot\boldsymbol{y}$; $(\lambda+\psi)\cdot\boldsymbol{x}=\lambda\cdot\boldsymbol{x}+\psi\cdot\boldsymbol{x}$
        \item Associativity (outer operation) \hfill $\lambda\cdot(\psi\cdot\boldsymbol{x})=(\lambda\psi)\cdot\boldsymbol{x}$
        \item Neutral element (outer operation) \hfill $\forall\boldsymbol{x}\in\mathcal{V}:1\cdot\boldsymbol{x}=\boldsymbol{x}$
    \end{itemize}
\end{cheatsheetblock}

\begin{cheatsheetblock}{Subspace}
    \begin{itemize}
        \item subset of vector space $V$
        \item Does it satisfy $\mathcal{U} \neq \varnothing$, in particular $0 \in \mathcal{U}$
        \item closure for both inner $x + u$ and outer operation $\lambda x$.
    \end{itemize}
\end{cheatsheetblock}

\begin{cheatsheetblock}{Linear dependence}
    Exists non-trivial linear combination $\mathbf{0}=\sum_{i=1}^k \lambda_i \boldsymbol{x}_i$.
\end{cheatsheetblock}

\begin{cheatsheetblock}{Linear Mapping}
    $\Phi(\lambda x+\psi y)=\lambda \Phi(x)+\psi \Phi(y)$

    Bilinear mapping:\hfill $\Omega(\lambda \boldsymbol{x}+\varphi \boldsymbol{y}, \boldsymbol{z})=\lambda \Omega(\boldsymbol{x}, \boldsymbol{z})+\varphi \Omega(\boldsymbol{y}, \boldsymbol{z})$

    \hfill$\Omega(\boldsymbol{x}, \lambda \boldsymbol{y}+\varphi \boldsymbol{z})=\lambda \Omega(\boldsymbol{x}, \boldsymbol{y})+\varphi \Omega(\boldsymbol{x}, \boldsymbol{z})$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Inner product}
    \begin{itemize}
        \item bilinear mapping, \hfill $\Omega: V \times V \rightarrow \mathbb{R}$
        \item symmetric, \hfill $\Omega(x, y)=\Omega(y, x)$
        \item positive definite, \hfill $\forall x \in V \backslash\{\mathbf{0}\}: \Omega(\boldsymbol{x}, \boldsymbol{x})>0, \Omega(\mathbf{0}, \mathbf{0})=0$
    \end{itemize}
\end{cheatsheetblock}

\begin{cheatsheetblock}{Orthogonal and Angle}
    Orthogonal: \hfill $x \perp y \Leftrightarrow \langle x, y\rangle=0$

    Orth Complement: \hfill $W^\perp = \{ v \in V : \langle v, w \rangle = 0 \text{ for all } w \in W \}$

    Angle: \hfill $\cos \omega=\frac{\langle x, y\rangle}{\sqrt{\langle x, x\rangle\langle y, y\rangle}}$

    Orthogonal matrices: \hfill $\bm{A}^{\top} = \bm{A}^{-1}$

    Projection: linear mapping $\pi: V \rightarrow U$ that \hfill $\pi^{2}=\pi$

    \hfill $\operatorname{proj}_{\boldsymbol{U}} \boldsymbol{x}=\frac{\langle \boldsymbol{x}, \boldsymbol{u}\rangle}{\langle \boldsymbol{u}, \boldsymbol{u}\rangle} \boldsymbol{u}$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Norm}
    \begin{itemize}
        \item $\|\lambda x\|=|\lambda| \|x\|$ \hfill Absolutely homogeneous
        \item $\|x+y\| \leq\|x\|+\|y\|$ \hfill Triangle inequality
        \item $\|x\| \geq 0$ \hfill Positive definite
    \end{itemize}
\end{cheatsheetblock}

\begin{cheatsheetblock} {Vector Calculus}
    $f^{\prime}(x)=\lim _{h \rightarrow 0} \frac{f(x+h)-f(x)}{h}$; \hfill $\frac{\partial f}{\partial x}=\lim _{h \rightarrow 0} \frac{f(x+h, y)-f(x, y)}{h}$

    \begin{itemize}
        \item $(f g)^{\prime}=f^{\prime} g+f g^{\prime}$ \hfill \text{Product rule}
        \item $\left(\frac{f}{g}\right)^{\prime}=\frac{f^{\prime} g-f g^{\prime}}{g^{2}}$ \hfill \text{Quotient rule}
        \item $(f \circ g)^{\prime}(x)=f^{\prime}(g(x)) g^{\prime}(x)$ \hfill \text{Chain rule}
        \item $(f+g)^{\prime}=f^{\prime}+g^{\prime}$ \hfill \text{Sum rule}
    \end{itemize}
\end{cheatsheetblock}

\begin{cheatsheetblock}{Gradients}
    $        \nabla_x f=\operatorname{grad} f=\frac{\mathrm{d} f}{\mathrm{~d} \boldsymbol{x}}=\left[\begin{array}{lll}
                \frac{\partial f(\boldsymbol{x})}{\partial x_1} & \ldots & \frac{\partial f(\boldsymbol{x})}{\partial x_n}
            \end{array}\right] \in \mathbb{R}^{1 \times n}$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Derivative Formula}
    $\sin(x)^{\prime}=\cos (x)$ \hfill $\cos (x)^{\prime}=-\sin (x)$ \hfill $(\tan x)^{\prime}=1 / \cos ^{2} x$

    $\ln (x)^{\prime}=1 / x$  \hfill $(e^{x})^{\prime}=e^{x}$ \hfill $(a^{x})^{\prime}=a^{x} \ln (a)$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Integration Formula (all have $+C$)}
    $a \to ax$ \hfill $x^n \to \frac{x^{n+1}}{n+1}$ \hfill $\frac{1}{x} \to \ln|x|$ \hfill $a^x \to \frac{a^x}{\ln a}$

    $\frac{1}{\sqrt{1-x^2}} \to \sin^{-1} x$ \hfill $\frac{1}{1+x^2} \to \tan^{-1} x$
\end{cheatsheetblock}


\begin{cheatsheetblock}{partial differentiation rules}
    $\frac{\partial}{\partial x}(f(\boldsymbol{x}) g(\boldsymbol{x}))=\frac{\partial f}{\partial x} g(\boldsymbol{x})+f(\boldsymbol{x}) \frac{\partial g}{\partial \boldsymbol{x}}$ \hfill \text{Product rule}

    $\frac{\partial}{\partial \boldsymbol{x}}(f(\boldsymbol{x})+g(\boldsymbol{x}))=\frac{\partial f}{\partial \boldsymbol{x}}+\frac{\partial g}{\partial \boldsymbol{x}}$ \hfill \text{Sum rule}
    
    $\frac{\partial}{\partial x}(g \circ f)(x)=\frac{\partial}{\partial x}(g(f(x)))=\frac{\partial g}{\partial f} \frac{\partial f}{\partial x}$ \hfill \text{Chain rule}

    $\frac{d f}{d t}=\frac{\partial f}{\partial x_1} \frac{\partial x_1}{\partial t}+\frac{\partial f}{\partial x_2} \frac{\partial x_2}{\partial t}$     \hfill \text{Chain rule, n input}
    
    $\frac{\partial \bm{x}^\top \bm{a}}{\partial \bm{x}} = \bm{a}^\top$ \hfill $\frac{\partial \bm{a}^\top \bm{x}}{\partial \bm{x}} = \bm{a}^\top$ \hfill $\frac{\partial \bm{a}^\top \bm{X} \bm{b}}{\partial \bm{X}} = \bm{a} \bm{b}^\top$

    $\frac{\partial \bm{x}^\top \bm{B} \bm{x}}{\partial \bm{x}} = \bm{x}^\top (\bm{B} + \bm{B}^\top)$

    $\frac{\partial}{\partial \bm{s}} (\bm{x} - \bm{A}\bm{s})^\top \bm{W} (\bm{x} - \bm{A}\bm{s}) = -2 (\bm{x} - \bm{A}\bm{s})^\top \bm{WA}$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Vector Calculus}
    $\frac{f: \mathbb{R}^k \rightarrow \mathbb{R}}{\bm{x} \in \mathbb{R}^k}$: \hfill
    $\frac{d\,f(\bm{x})}{d\,\bm{x}}=\left[\begin{array}{cccc}
                \frac{\partial f(\bm{x})}{\partial x_1} & \frac{\partial f(\bm{x})}{\partial x_2} & \cdots & \frac{\partial f(\bm{x})}{\partial x_k}
            \end{array}\right] \in \mathbb{R}^{1 \times k}$

    $\frac{f: \mathbb{R} \rightarrow \mathbb{R}^k / \mathbb{R}^{1 \times k}}{x \in \mathbb{R}}$: \hfill
    $\frac{d\left[\begin{array}{ll}
                    \sin x & \cos x
                \end{array}\right]}{d x}=\left[\begin{array}{l}
                \frac{d \sin x}{d x} \\
                \frac{d \cos x}{d x}
            \end{array}\right]=\left[\begin{array}{c}
                \cos x \\
                -\sin x
            \end{array}\right]$

    $\frac{f: \mathbb{R} \rightarrow \mathbb{R}^k / \mathbb{R}^{1 \times k}}{x \in \mathbb{R}^m}$: \hfill
    apply 2, then apply 1; cal; convert back.

    $\frac{f: \mathbb{R}^{m \times k} \rightarrow \mathbb{R}}{\bm{X} \in \mathbb{R}^{m \times k}}$ \hfill
    $\frac{d\, \operatorname{tr}(\bm{X})}{d\, \bm{X}} = \begin{bmatrix}
            \frac{\partial \operatorname{tr} (\bm{X})}{\partial \bm{X}_{11}} & \frac{\partial \operatorname{tr} (\bm{X})}{\partial \bm{X}_{12}} \\
            \frac{\partial \operatorname{tr} (\bm{X})}{\partial \bm{X}_{21}} & \frac{\partial \operatorname{tr} (\bm{X})}{\partial \bm{X}_{22}}
        \end{bmatrix}$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Gaussian distribution}
    uni var: \hfill $\mathcal{N}\left(\mu, \sigma^2\right): \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$

    mul: \hfill $\mathcal{N}\left(\boldsymbol{\mu}, \boldsymbol{\Sigma}\right): (2\pi)^{-\frac{k}{2}}|\boldsymbol{\Sigma}|^{-\frac{1}{2}} \exp \left(-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right)$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Probability}
    $P(X)=\sum_{y \in \mathscr{Y}} P(X, Y=y)$ \hfill sum rule

    $P(X, Y)=P(X \mid Y) P(Y)=P(Y \mid X) P(X)$ \hfill product rule

    $P(X \mid Y)=\frac{P(X, Y)}{P(Y)}=\frac{P(Y \mid X) P(X)}{P(Y)}$ \hfill Bayes' rule

    $P(a \leq X \leq b)=\int_{a}^{b} f(x) \mathrm{d} x$

    independent: $p(\mathbf{x}, \mathbf{y})=p(\mathbf{x}) p(\mathbf{y})$

\end{cheatsheetblock}

\begin{cheatsheetblock}{Bernoulli and Binomial distributions}
    $p(x=k)= {n \choose k} \times p^k (1-p)^{n-k}$ \hfill ${n \choose k} = \frac{n!}{k!(n-k)!}$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Expectation}
    $\mathbb{E}_X[g(x)]= \begin{cases}\sum_{x \in X} g(x) p(x) & \text { if } X \text { is discrete } \\ \int_\mathscr{X} g(x) p(x) \mathrm{d} x & \text { if } X \text { is continuous }\end{cases}$

    $\mathbb{E}[\bm{y}]=\mathbb{E}[\bm{A x}+\bm{b}]=\bm{A} \mathbb{E}[\bm{x}]+\bm{b}$

    $\mathbb{V}[\bm{y}]=\mathbb{V}[\bm{A} \bm{x}+\bm{b}]=\bm{A} \mathbb{V}[\bm{x}] \bm{A}^{\top}$

    $\operatorname{Cov}[\bm{x}, \bm{y}]=\mathbb{E}\left[\bm{x}(\bm{A} \bm{x}+\bm{b})^{\top}\right]-\mathbb{E}[\bm{x}] \mathbb{E}[(\bm{A} \bm{x}+\bm{b})]^{\top}=\mathbb{V}[\bm{x}] \bm{A}^{\top}$

    $\mathbb{V}[\mathbf{x} \pm \mathbf{y}]=\mathbb{V}[\mathbf{x}]+\mathbb{V}[\mathbf{y}] \pm \operatorname{Cov}[\mathbf{x}, \mathbf{y}] \pm \operatorname{Cov}[\mathbf{y}, \mathbf{x}]$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Linear Regression}
    Loss \hfill $L(\theta)=\frac{1}{N} \sum_{n=1}^N\left(y_n-f_\theta\left(\mathbf{x}_n\right)\right)^2=\frac{1}{N}\|\mathbf{y}-X \theta\|_2^2$

    $=\frac{1}{N}(\mathbf{y}-X \theta)^{\top}(\mathbf{y}-X \theta)$ \hfill $\implies$ \hfill $\theta=\left(X^{\top} X\right)^{-1} X^{\top} \mathbf{y}$

    Regularised: \hfill $L(\theta)=\frac{1}{N}\|\mathbf{y}-X \theta\|_2^2+\lambda\|\theta\|_2^2$

    $=\frac{1}{N}(\mathbf{y}-X \theta)^{\top}(\mathbf{y}-X \theta)+\lambda\|\theta\|_2^2$ \hfill $\implies$ \hfill $\theta=\left(X^{\top} X+N \lambda \mathbf{I}\right)^{-1} X^{\top} \mathbf{y}$

    Probabilistic: \hfill $P(\theta \mid \mathscr{D})=\frac{P(\theta, \mathscr{D})}{P(\mathscr{D})}=\frac{P(\theta) P(\mathscr{D} \mid \theta)}{P(\mathscr{D})}$

    \hfill $\text{Posterior} = \frac{\text{Prior} \times \text{Likelihood}}{\text{Marginal likelihood}}$
\end{cheatsheetblock}

\begin{cheatsheetblock}{determinant}
    $\det(\bm{A}) = \sum_{k=1}^{n} (-1)^{k+i} a_{i,k} \det(\bm{A}_{\setminus i,k})$ \hfill Laplace expansion

    $R_1 \Leftrightarrow R_2 \implies -\det(\bm{A})$ \hfill $R_1 := R_1 - R_2 \implies \det(\bm{A})$

    $\det (\bm{A} \bm{B}) = \det (\bm{A}) \det (\bm{B})$ \hfill $\det (\bm{A}^{\top}) = \det (\bm{A})$

    $\det (\bm{A}^{-1}) = \frac{1}{\det (\bm{A})}$ \hfill $\det (\bm{A}^n) = \det (\bm{A})^n$

    $\begin{bmatrix}
            t a & t b \\
            c   & d
        \end{bmatrix}=t\begin{bmatrix}
            a & b \\
            c & d
        \end{bmatrix}$
    \hfill
    $\begin{bmatrix}
            a+a^{\prime} & b+b^{\prime} \\
            c            & d
        \end{bmatrix}=\begin{bmatrix}
            a & b \\
            c & d
        \end{bmatrix}+\begin{bmatrix}
            a^{\prime} & b^{\prime} \\
            c          & d
        \end{bmatrix}$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Trace}
    $\operatorname{tr}(\bm{A}+\bm{B})=\operatorname{tr}(\bm{A})+\operatorname{tr}(\bm{B})$ \hfill $\operatorname{tr}(\alpha \bm{A})=\alpha \operatorname{tr}(\bm{A})$

    $\operatorname{tr}(\bm{A} \bm{B})=\operatorname{tr}(\bm{B} \bm{A})$ \hfill $\operatorname{tr}(\bm{A}^{\top})=\operatorname{tr}(\bm{A})$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Eigenvalues and Eigenvectors}
    $p_{\bm{A}}(\lambda):=\det(\bm{A}-\lambda \bm{I})$ \hfill characteristic polynomial

    solving $p_{\bm{A}}(\lambda)=0$ \hfill eigenvalues $\lambda_1, \lambda_2, \ldots, \lambda_n$

    $\bm{A}\bm{x} = \lambda \bm{x}$ \hfill eigenvectors $\bm{x}_1, \bm{x}_2, \ldots, \bm{x}_n$

    $\det(\bm{A}) = \prod_{i=1}^n \lambda_i$ \hfill $\operatorname{tr}(\bm{A}) = \sum_{i=1}^n \lambda_i$

    $k$ distinct eigenvalues $\implies$ $k$ linearly independent eigenvectors

    $k < n$ $\implies$ $\bm{A}$ is defective (not diagonalisable)
\end{cheatsheetblock}

\begin{cheatsheetblock}{Eigenvalues and Eigenvectors Equivalence}
    \begin{itemize}
        \item $\lambda$ is eigenvalue of $\bm{A}$
        \item $\exists \bm{x} \neq \bm{0}: \bm{A} \bm{x} = \lambda \bm{x}$
        \item $(\bm{A} - \lambda \bm{I}) \bm{x} = \bm{0}$ have non-trivial solution
        \item $\operatorname{rk}(\bm{A} - \lambda \bm{I}_n) < n$
        \item $\det (\bm{A} - \lambda \bm{I}) = 0$
    \end{itemize}
\end{cheatsheetblock}

\begin{cheatsheetblock}{Eigen-decomposition}
    $\bm{A}, \bm{B}$ are similar $:=$ $B = \bm{P}^{-1} \bm{A} \bm{P}$ \hfill $\implies$ \hfill same eigenvalues.

    $\bm{A}$ is diagonalisable $:=$ $\bm{A}$ is similar to a diagonal matrix.

    \hfill that is, $\bm{D} = \bm{P}^{-1} \bm{A} \bm{P}$

    $\bm{P}$ columns are eigenvectors \hfill diagonal matrix $\bm{D}$  eigenvalues.

    A square, symmetric matrix $\implies$ diagonalisable.
\end{cheatsheetblock}

\begin{cheatsheetblock}{Singular Value Decomposition}
    $A=U \Sigma V^{\top}$

    $\bm{A}^{\mathrm{T}} \bm{A}=\bm{P} \bm{D} \bm{P}^{\mathrm{T}}$: \hfill $\bm{V} = \bm{P}$, $\bm{D} = \bm{\Sigma}^2$ \hfill col of $P$ need normalized.
    $\bm{u}_i = \frac{1}{\sigma_i} \bm{A} \bm{v}_i$

\end{cheatsheetblock}

\begin{cheatsheetblock}{Gaussian mixture models (GMM)}
    $L(\theta)=\log p(X \mid \theta)=\sum_{n=1}^N \log \sum_{k=1}^K \pi_k \mathcal{N}\left(x_n ; \mu_k, \Sigma_k\right)$

    $\frac{\partial L}{\partial \mu_k}, \frac{\partial L}{\partial \Sigma_k} , \frac{\partial L}{\partial \pi_k}$ all set to 0.

    Evaluate responsibilities: \quad $r_{n k}=\frac{\pi_k \mathcal{N}\left(x_n ; \mu_k, \Sigma_k\right)}{\sum_j \pi_j \mathcal{N}\left(x_n ; \mu_j, \Sigma_j\right)}$

    Re-estimate parameters:

    $\mu_k=\frac{1}{N_k} \sum_{n=1}^N r_{n k} x_n$ \hfill $N_k=\sum_{n=1}^N r_{n k}$ \hfill $\pi_k = \frac{N_k}{N}$

    $\Sigma_k=\frac{1}{N_k} \sum_{n=1}^N r_{n k}\left(x_n-\mu_k\right)\left(x_n-\mu_k\right)^{\top}$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Classification}
    $\sigma(x) = \frac{\exp(x)}{1 + \exp(x)} = \frac{1}{1 + e^{-x}}$ \hfill logistic sigmoid

    $p(y_n \mid x_n, \theta)=g_\theta(x_n)$ if $y_n=1$;\quad $1-g_\theta(x_n)$ if $y_n=0$

    $\mathscr{L}_n(\theta)=-y_n \log \left(g_\theta\left(x_n\right)\right)-\left(1-y_n\right) \log \left(1-g_\theta\left(x_n\right)\right)$

    $\frac{\mathrm{d} \mathscr{L}(\theta)}{\mathrm{d} \theta}=\frac{1}{N} \sum_{n=1}^N\left(g_\theta\left(x_n\right)-y_n\right) x_n^T$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Classification (multi-class)}
    $g_\theta^{(c)}(x)=\operatorname{softmax}\left(f_\theta^{(c)}(x)\right)=\frac{\exp \left(f_\theta^{(c)}(x)\right)}{\sum_{k=1}^C \exp \left(f_\theta^{(k)}(x)\right)}$

    $p\left(y_n=c \mid x_n, \theta\right)=g_\theta^{(c)}\left(x_n\right)$; $\mathscr{L}(\theta)=-\frac{1}{N} \sum_{n=1}^N \log \left(g_\theta^{\left(y_n\right)}\left(x_n\right)\right)$

    $\mathscr{L}(\theta)=-\frac{1}{N} \sum_{n=1}^N \sum_{c=1}^C t_{n c} \log \left(g_\theta^c\left(x_n\right)\right)$ \hfill one-hot coding

    $\frac{\mathrm{d} \mathscr{L}(\theta)}{\mathrm{d} \theta_c}=\frac{1}{N} \sum_{n=1}^N\left(g_\theta^{(c)}\left(x_n\right)-t_{n c}\right) x_n^T$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Perceptron model}
    $y_n = 1$, if $\theta^\top x_n \geq 0$; $-1$, $\theta^\top x_n < 0$ \hfill $L(\theta)=-\sum_{n \in \mathscr{M}} y_n \theta^T x_n$

    works only linearly separable.

    Accuracy $=\frac{\mathrm{TP}+\mathrm{TN}}{N} \quad$ Precision $=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}}$

    Error $=\frac{\mathrm{FP}+\mathrm{FN}}{N} \quad$ Recall $=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}} \quad$

    True positive rate $(\mathrm{TPR})=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}}$ \hfill False $(\mathrm{FPR})=\frac{\mathrm{FP}}{\mathrm{FP}+\mathrm{TN}}$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Linear Regression}
    $L(\theta)=\frac{1}{N} \sum_{n=1}^N\left(y_n-f_\theta\left(\mathbf{x}_n\right)\right)^2$ \hfill Least squares

    $\theta=\left(X^{\top} X\right)^{-1} X^{\top} \mathbf{y}$

    $f_\theta(\mathbf{x})=\sum_{d=1}^D \theta_d \phi(x)_d=\theta^{\top} \phi(\mathbf{x})$ \hfill with features

    $L(\theta)=\frac{1}{N} \sum_{n=1}^N\left(y_n-f_\theta\left(\mathbf{x}_n\right)\right)^2$ \hfill $\theta=\left(\Phi^{\top} \Phi\right)^{-1} \Phi^{\top} \mathbf{y}$

    $L(\theta)=\frac{1}{N}\|\mathbf{y}-X \theta\|_2^2+\lambda\|\theta\|_2^2$ \hfill Regularised

    $\theta=\left(X^{\top} X+N \lambda \mathrm{I}\right)^{-1} X^{\top} \mathbf{y}$
\end{cheatsheetblock}

\begin{cheatsheetblock}{Principal Component Analysis (PCA)}
    \begin{enumerate}
        \item standardize data $\frac{x-\mu}{\sigma}$
        \item eigendecomposition of covariance matrix $\frac{1}{N}XX^\top$
        \item select largest $k$ eigenvalues and corresponding eigenvectors to form projection matrix $\bm{B}$. The project is $\tilde{\bm{X}} = \bm{B}\bm{B}^\top \bm{X}$
        \item undo standardization $\tilde{\bm{X}} \sigma + \mu$ to get dimension-reduced data.
    \end{enumerate}
\end{cheatsheetblock}

\end{multicols}
\end{document}
